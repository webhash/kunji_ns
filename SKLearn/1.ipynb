{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets play around with sklearn \n",
    "\n",
    "# reference https://scikit-learn.org/stable/getting_started.html#getting-started\n",
    "\n",
    "# Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. \n",
    "# It also provides various tools for model fitting, data preprocessing, model selection and evaluation, \n",
    "# and many other utilities.\n",
    "\n",
    "# few keywords to know before we start \n",
    "\n",
    "# What is Machine Learning?\n",
    "\n",
    "# Machine learning is a subset of artificial intelligence (AI) in which algorithms learn by example \n",
    "# from historical data to predict outcomes and uncover patterns not easily spotted by humans. \n",
    "\n",
    "# What is Model Fitting?\n",
    "# Model fitting is a measure of how well a machine learning model generalizes \n",
    "# to similar data to that on which it was trained. \n",
    "# A model that is well-fitted produces more accurate outcomes.\n",
    "# A model that is overfitted matches the data too closely. \n",
    "# A model that is underfitted doesn’t match closely enough.\n",
    "\n",
    "# What does Prediction mean in Machine Learning?\n",
    "# “Prediction” refers to the output of an algorithm after it has been trained on a historical dataset \n",
    "# and applied to new data when forecasting the likelihood of a particular outcome, \n",
    "# The algorithm will generate probable values for an unknown variable for each record in the new data, \n",
    "# allowing the model builder to identify what that value will most likely be.\n",
    "\n",
    "# Cross-validation is an extension of the training, validation, \n",
    "# and holdout (TVH) process that minimizes the sampling bias of machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and predicting: estimator basics\n",
    "\n",
    "# Scikit-learn provides dozens of built-in machine learning algorithms and models, called estimators. \n",
    "# Each estimator can be fitted to some data using its fit method.\n",
    "\n",
    "# we fit a RandomForestClassifier to some very basic data:\n",
    "\n",
    "# what is random forest classifier \n",
    "# Random forests or random decision forests are an ensemble learning method for classification, \n",
    "# regression and other tasks that operate by constructing a multitude of decision trees at training time \n",
    "# and outputting the class that is the mode of the classes (classification) \n",
    "# or mean/average prediction (regression)of the individual trees.\n",
    "\n",
    "# what is a ensemble learning method ?\n",
    "# In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n",
    "# Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, \n",
    "# but typically allows for much more flexible structure to exist among those alternatives.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[ 1,  2,  3, 4], [11, 12, 13, 14], [21, 22, 23, 24]] # 3 samples, 4 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0, 1, 2] # classes for each sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The fit method generally accepts 2 inputs:\n",
    "\n",
    "The samples matrix (or design matrix) X. \n",
    "The size of X is typically (n_samples, n_features), \n",
    "which means that samples are represented as rows and features are represented as columns.\n",
    "\n",
    "The target values y which are real numbers for regression tasks, or integers for classification \n",
    "(or any other discrete set of values). For unsupervized learning tasks, \n",
    "y does not need to be specified. \n",
    "y is usually 1d array where the i th entry corresponds to the target of the i th sample (row) of X.\n",
    "'''\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on training data shall return us the expected values\n",
    "clf.predict(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on the new data \n",
    "clf.predict([[4, 5, 6, 9], [14, 15, 16, 19], [24, 25, 26, 29] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers and pre-processors \n",
    "\n",
    "# Machine learning workflows are often composed of different parts. \n",
    "# A typical pipeline consists of a pre-processing step that transforms or imputes the data, \n",
    "# and a final predictor that predicts target values.\n",
    "\n",
    "# pre-processors and transformers follow the same API as the estimator objects \n",
    "# (they actually all inherit from the same BaseEstimator class). \n",
    "# The transformer objects don’t have a predict method but rather a transform method that\n",
    "# outputs a newly transformed sample matrix X:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = [ [0,2,3], [-1,0,10], [9,-10,-10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.59299945,  0.88900089,  0.2413554 ],\n",
       "       [-0.81537425,  0.50800051,  1.08609928],\n",
       "       [ 1.4083737 , -1.3970014 , -1.32745468]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the mean and std to be used for later scaling.\n",
    "fx = StandardScaler().fit(X)\n",
    "fx.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.66666667, -2.66666667,  1.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.29649973, -0.44450044,  0.60338849],\n",
       "       [-3.70624658,  3.36550337,  1.81016547],\n",
       "       [ 4.74399563,  5.27050527, -3.25829785]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.transform([[4, -5, 6], [-14, 15, 16], [24, 25, -26] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines: chaining pre-processors and estimators\n",
    "\n",
    "# Transformers and estimators (predictors) can be combined together into a single unifying object: a Pipeline. \n",
    "# The pipeline offers the same API as a regular estimator: it can be fitted and used for prediction with fit and predict. \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline object\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(random_state=0)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset and split it into train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression(random_state=0))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the whole pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# we can now use it like any other estimator\n",
    "accuracy_score(pipe.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "# Fitting a model to some data does not entail that it will predict well on unseen data. \n",
    "# This needs to be directly evaluated. \n",
    "# We have just seen the train_test_split helper that splits a dataset into train and test sets, \n",
    "# but scikit-learn provides many other tools for model evaluation, in particular for cross-validation.\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41929687, -1.5489299 ,  0.65218686, ..., -0.81368398,\n",
       "        -2.03884275,  0.90000294],\n",
       "       [-2.06947249,  0.72712806,  0.0975975 , ..., -0.35978104,\n",
       "        -0.74513907, -0.55050613],\n",
       "       [-0.37595997,  0.66414405,  1.02239232, ...,  0.50481546,\n",
       "        -2.83201187, -0.79978614],\n",
       "       ...,\n",
       "       [-0.7719197 , -1.33667649, -0.72733814, ..., -0.59830311,\n",
       "        -0.60986158,  1.69242973],\n",
       "       [ 0.67198393, -1.50733364,  1.17622157, ...,  2.05921537,\n",
       "        -1.11140442,  0.01787532],\n",
       "       [ 1.10334268, -0.59531919, -0.29831814, ..., -0.89706521,\n",
       "        -0.11546748, -1.299286  ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_regression(n_samples=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.01299667, 0.008919  , 0.00655627, 0.00681496, 0.00844073]),\n",
       " 'score_time': array([0.00229025, 0.0009501 , 0.00089979, 0.00081778, 0.00110316]),\n",
       " 'test_score': array([1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = cross_validate(lr, X, y)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic parameter searches \n",
    "\n",
    "# All estimators have parameters (often called hyper-parameters in the literature) that can be tuned. \n",
    "# The generalization power of an estimator often critically depends on a few parameters. \n",
    "# Quite often, it is not clear what the exact values of these parameters should be since they depend on the data at hand.\n",
    "\n",
    "# Scikit-learn provides tools to automatically find the best parameter combinations (via cross-validation).\n",
    "\n",
    "# In the following example, we randomly search over the parameter space of a random forest with a RandomizedSearchCV object. \n",
    "# When the search is over, the RandomizedSearchCV behaves as a RandomForestRegressor that has been fitted with the best set of parameters. \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter space that will be searched over\n",
    "param_distributions = {'n_estimators': randint(1, 5),\n",
    "                       'max_depth': randint(5, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a searchCV object and fit it to the data\n",
    "search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),\n",
    "                            n_iter=5,\n",
    "                            param_distributions=param_distributions,\n",
    "                            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0), n_iter=5,\n",
       "                   param_distributions={'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8a29435af0>,\n",
       "                                        'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8a29678d90>},\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 9, 'n_estimators': 4}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the best parameters \n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.735363411343253"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the search object now acts like a normal random forest estimator\n",
    "# with max_depth=9 and n_estimators=4\n",
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
